{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "304a4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24178c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"D:\\_Files\\Shen_Lab\\Cytometry\\omiq_exported_data_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f06972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_extrema(dir_name):\n",
    "    \"\"\"Return the global maxima and minima of Ir191Di___191Ir_DNA1 and\n",
    "       Event_length in all the files under the given directory.\"\"\"\n",
    "    # The following helper function returns the relavant maxima and minima\n",
    "    # for a specific file\n",
    "    def local_extrema(filename):\n",
    "        data = pd.read_csv(os.path.join(dir_name, filename))\n",
    "        ir191 = data[\"Ir191Di___191Ir_DNA1\"]\n",
    "        el = data[\"Event_length\"]\n",
    "        return (ir191.max(), ir191.min(), el.max(), el.min())\n",
    "    ir191_max, el_max = -math.inf, -math.inf\n",
    "    ir191_min, el_min = math.inf, math.inf\n",
    "    # find the global maxima and minima for the two parameters across\n",
    "    # all the files in the directory\n",
    "    for file in os.listdir(dir_name):\n",
    "        ir191_max_l, ir191_min_l, el_max_l, el_min_l = local_extrema(file)\n",
    "        ir191_max = max(ir191_max, ir191_max_l)\n",
    "        ir191_min = min(ir191_min, ir191_min_l)\n",
    "        el_max = max(el_max, el_max_l)\n",
    "        el_min = min(el_min, el_min_l)\n",
    "    return (ir191_max, ir191_min, el_max, el_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f92895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ir191_max = 10.263\n",
      " ir191_min = 0.0\n",
      " el_max = 174\n",
      " el_min = 10\n"
     ]
    }
   ],
   "source": [
    "ir191_max, ir191_min, el_max, el_min = global_extrema(root)\n",
    "print(f\" ir191_max = {ir191_max}\\n ir191_min = {ir191_min}\\n el_max = {el_max}\\n el_min = {el_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e373343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def axes_convert(df_numpy):\n",
    "    \"\"\"Convert the axes according to the following rules:\n",
    "       ir191: int(ir191 * 10000 // 622)\n",
    "       el: int(el - 10)\"\"\"\n",
    "    for idx in range(df_numpy.shape[0]):\n",
    "        ir191, el, gate1 = df_numpy[idx]\n",
    "        ir191 = ir191 * 10000 // 622\n",
    "        el = el - 10\n",
    "        gate1 = gate1\n",
    "        df_numpy[idx] = np.array((ir191, el, gate1))\n",
    "    return df_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb03af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_image(filename, highlight=False):\n",
    "    \"\"\"Read a data file and output a shrinked greyscale image for the dataset.\n",
    "       The output image is a numpy array of shape (166, 166, 1).\n",
    "       To visualise the gate, set highlight to True. For training purpose, set to False.\n",
    "       \"\"\"\n",
    "    image = np.zeros((166, 166, 1))\n",
    "    data = pd.read_csv(os.path.join(root, filename))[[\"Ir191Di___191Ir_DNA1\", \"Event_length\", \"gate1_ir\"]]\n",
    "    data = axes_convert(data.to_numpy())\n",
    "    # for each cell in the dataset, we convert the axes by the rules described above\n",
    "    for cell in data:\n",
    "        ir191, el, gate1 = cell\n",
    "        # need to make sure these variables are integers\n",
    "        ir191, el, gate1 = int(ir191), int(el), int(gate1)\n",
    "        if highlight:\n",
    "            if gate1 == 0:\n",
    "                image[ir191, el, 0] = 128\n",
    "            elif gate1 == 1:\n",
    "                image[ir191, el, 0] = 255\n",
    "        else:\n",
    "            image[ir191, el, 0] += 1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b18fc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = convert_to_image(\"49.T1_Normalized.csv\")\n",
    "plt.imshow(test_image, cmap=\"gray\")\n",
    "del test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36d3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convex_hull_vertices(filename):\n",
    "    \"\"\"Read a data file and return a list of 10 vertices that define a polygon\n",
    "       approximating the convex hull (heptagon) of the in-gate data points.\"\"\"\n",
    "    # The following code segment computes the convex hull\n",
    "    data = pd.read_csv(os.path.join(root, filename))[[\"Ir191Di___191Ir_DNA1\", \"Event_length\", \"gate1_ir\"]]\n",
    "    in_gate1 = data[data[\"gate1_ir\"] == 1]\n",
    "    in_gate1 = in_gate1[[\"Ir191Di___191Ir_DNA1\", \"Event_length\"]].to_numpy()\n",
    "    hull = ConvexHull(in_gate1)\n",
    "    # The following code segment extracts the vertices of the convex hull\n",
    "    vertices = []\n",
    "    for vertex in hull.vertices:\n",
    "        vertices.append(tuple(in_gate1[vertex]))\n",
    "    # The following code segment reduces the number of vertices to 10\n",
    "    vertex_slope_diff = {}\n",
    "    num_vertices = len(vertices)\n",
    "    for j in range(num_vertices):\n",
    "        i, k = (j - 1) % num_vertices, (j + 1) % num_vertices\n",
    "        slope1 = (vertices[j][1] - vertices[i][1]) / (vertices[j][0] - vertices[i][0])\n",
    "        slope2 = (vertices[k][1] - vertices[j][1]) / (vertices[k][0] - vertices[j][0])\n",
    "        vertex_slope_diff = dict(sorted(vertex_slope_diff.items()))\n",
    "        reduced_vertices_idx = list(vertex_slope_diff.values())[-10:]\n",
    "        reduced_vertices = []\n",
    "        for idx in reduced_vertices_idx:\n",
    "            reduced_vertices.append(vertices[idx])\n",
    "    return reduced_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece7fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(root):\n",
    "    image = convert_to_image(f, True)\n",
    "    filename = f[:-4]\n",
    "    np.save(f\"D:\\_Files\\Shen_Lab\\Cytometry\\gate_1\\plots\\highlighted\\{filename}.npy\", image)\n",
    "    del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46046cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(root):\n",
    "    image = convert_to_image(f, False)\n",
    "    filename = f[:-4]\n",
    "    np.save(f\"D:\\_Files\\Shen_Lab\\Cytometry\\gate_1\\plots\\\\train\\{filename}.npy\", image)\n",
    "    del image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9bb3a",
   "metadata": {},
   "source": [
    "## ResNet50 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37983cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(block, layers, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(Bottleneck, [3, 4, 6, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a67adfc",
   "metadata": {},
   "source": [
    "## FCN_ResNet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbc0d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class IntermediateLayerGetter(nn.ModuleDict):\n",
    "    \"\"\"\n",
    "    Module wrapper that returns intermediate layers from a model\n",
    "    It has a strong assumption that the modules have been registered\n",
    "    into the model in the same order as they are used.\n",
    "    This means that one should **not** reuse the same nn.Module\n",
    "    twice in the forward if you want this to work.\n",
    "    Additionally, it is only able to query submodules that are directly\n",
    "    assigned to the model. So if `model` is passed, `model.feature1` can\n",
    "    be returned, but not `model.feature1.layer2`.\n",
    "    Args:\n",
    "        model (nn.Module): model on which we will extract the features\n",
    "        return_layers (Dict[name, new_name]): a dict containing the names\n",
    "            of the modules for which the activations will be returned as\n",
    "            the key of the dict, and the value of the dict is the name\n",
    "            of the returned activation (which the user can specify).\n",
    "    \"\"\"\n",
    "    _version = 2\n",
    "    __annotations__ = {\n",
    "        \"return_layers\": Dict[str, str],\n",
    "    }\n",
    "\n",
    "    def __init__(self, model: nn.Module, return_layers: Dict[str, str]) -> None:\n",
    "        if not set(return_layers).issubset([name for name, _ in model.named_children()]):\n",
    "            raise ValueError(\"return_layers are not present in model\")\n",
    "        orig_return_layers = return_layers\n",
    "        return_layers = {str(k): str(v) for k, v in return_layers.items()}\n",
    "\n",
    "        # 重新构建backbone，将没有使用到的模块全部删掉\n",
    "        layers = OrderedDict()\n",
    "        for name, module in model.named_children():\n",
    "            layers[name] = module\n",
    "            if name in return_layers:\n",
    "                del return_layers[name]\n",
    "            if not return_layers:\n",
    "                break\n",
    "\n",
    "        super(IntermediateLayerGetter, self).__init__(layers)\n",
    "        self.return_layers = orig_return_layers\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        out = OrderedDict()\n",
    "        for name, module in self.items():\n",
    "            x = module(x)\n",
    "            if name in self.return_layers:\n",
    "                out_name = self.return_layers[name]\n",
    "                out[out_name] = x\n",
    "        return out\n",
    "\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Fully-Convolutional Network for semantic segmentation.\n",
    "    Args:\n",
    "        backbone (nn.Module): the network used to compute the features for the model.\n",
    "            The backbone should return an OrderedDict[Tensor], with the key being\n",
    "            \"out\" for the last feature map used, and \"aux\" if an auxiliary classifier\n",
    "            is used.\n",
    "        classifier (nn.Module): module that takes the \"out\" element returned from\n",
    "            the backbone and returns a dense prediction.\n",
    "        aux_classifier (nn.Module, optional): auxiliary classifier used during training\n",
    "    \"\"\"\n",
    "    __constants__ = ['aux_classifier']\n",
    "\n",
    "    def __init__(self, backbone, classifier, aux_classifier=None):\n",
    "        super(FCN, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = classifier\n",
    "        self.aux_classifier = aux_classifier\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        input_shape = x.shape[-2:]\n",
    "        # contract: features is a dict of tensors\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        result = OrderedDict()\n",
    "        x = features[\"out\"]\n",
    "        x = self.classifier(x)\n",
    "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
    "        result[\"out\"] = x\n",
    "\n",
    "        if self.aux_classifier is not None:\n",
    "            x = features[\"aux\"]\n",
    "            x = self.aux_classifier(x)\n",
    "            x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
    "            result[\"aux\"] = x\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class FCNHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, channels):\n",
    "        inter_channels = in_channels // 4\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(inter_channels, channels, 1)\n",
    "        ]\n",
    "\n",
    "        super(FCNHead, self).__init__(*layers)\n",
    "\n",
    "\n",
    "def fcn_resnet50(aux, num_classes=2, pretrain_backbone=False):\n",
    "    # 'resnet50_imagenet': 'https://download.pytorch.org/models/resnet50-0676ba61.pth'\n",
    "    # 'fcn_resnet50_coco': 'https://download.pytorch.org/models/fcn_resnet50_coco-1167a1af.pth'\n",
    "    backbone = resnet50(replace_stride_with_dilation=[False, True, True])\n",
    "\n",
    "    if pretrain_backbone:\n",
    "        backbone.load_state_dict(torch.load(\"resnet50.pth\", map_location='cpu'))\n",
    "\n",
    "    out_inplanes = 2048\n",
    "    aux_inplanes = 1024\n",
    "\n",
    "    return_layers = {'layer4': 'out'}\n",
    "    if aux:\n",
    "        return_layers['layer3'] = 'aux'\n",
    "    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    aux_classifier = None\n",
    "    # why using aux: https://github.com/pytorch/vision/issues/4292\n",
    "    if aux:\n",
    "        aux_classifier = FCNHead(aux_inplanes, num_classes)\n",
    "\n",
    "    classifier = FCNHead(out_inplanes, num_classes)\n",
    "\n",
    "    model = FCN(backbone, classifier, aux_classifier)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9bd56",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "764fea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12603903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
